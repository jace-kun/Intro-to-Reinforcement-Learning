{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Qlearning Algorithm for OpenAI Gym Cartpole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import gym\n",
    "from collections import deque\n",
    "import itertools\n",
    "import numpy as np \n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCOUNT_FACTOR = 0.99\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 50000\n",
    "MIN_REPLAY_SIZE = 1000\n",
    "EXPLORE_RATE_START = 1.0\n",
    "EXPLORE_RATE_END = 0.02\n",
    "EXPLORE_RATE_DECAY = 10000\n",
    "TARGET_UPDATE_FREQ = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(2)\n",
      "State space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "Lower bounds of state space: [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n",
      "Upper bounds of state space: [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\") \n",
    "#env = gym.make(\"CartPole-v1\", render_mode= \"human\") # Use this one to visualize\n",
    "print(\"Action space:\", env.action_space)\n",
    "print(\"State space:\", env.observation_space)\n",
    "print(\"Lower bounds of state space:\", env.observation_space.low)\n",
    "print(\"Upper bounds of state space:\", env.observation_space.high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = deque(maxlen=BUFFER_SIZE)\n",
    "rew_buffer = deque([0.0], maxlen=100)\n",
    "\n",
    "episode_reward = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "\n",
    "        in_features = int(np.prod(env.observation_space.shape))\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, env.action_space.n)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    def act(self, obs):\n",
    "        obs_t = torch.as_tensor(obs, dtype=torch.float32)\n",
    "        q_values = self(obs_t.unsqueeze(0))\n",
    "\n",
    "        max_q_index = torch.argmax(q_values, dim=1)[0]\n",
    "        action = max_q_index.detach().item()\n",
    "\n",
    "        return action\n",
    "\n",
    "online_net = Network(env)\n",
    "target_net = Network(env)\n",
    "\n",
    "target_net.load_state_dict(online_net.state_dict())\n",
    "\n",
    "optimizer = torch.optim.Adam(online_net.parameters(), lr=5e4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "obs = obs[0]\n",
    "for _ in range(MIN_REPLAY_SIZE):\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    new_obs, rew, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    transition = (obs, action, rew, done, new_obs)\n",
    "    replay_buffer.append(transition)\n",
    "    obs = new_obs\n",
    "\n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "        obs = obs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 0\n",
      "Avg Rew 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 10000\n",
      "Avg Rew 19.45\n",
      "\n",
      "Step 20000\n",
      "Avg Rew 16.97\n",
      "\n",
      "Step 30000\n",
      "Avg Rew 23.61\n",
      "\n",
      "Step 40000\n",
      "Avg Rew 20.62\n",
      "\n",
      "Step 50000\n",
      "Avg Rew 43.47\n",
      "\n",
      "Step 60000\n",
      "Avg Rew 13.33\n",
      "\n",
      "Step 70000\n",
      "Avg Rew 16.06\n",
      "\n",
      "Step 80000\n",
      "Avg Rew 43.96\n",
      "\n",
      "Step 90000\n",
      "Avg Rew 21.24\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "obs = obs[0]\n",
    "\n",
    "for step in range(100000):\n",
    "    EXPLORE_RATE = np.interp(step, [0, EXPLORE_RATE_DECAY], [EXPLORE_RATE_START, EXPLORE_RATE_END])\n",
    "\n",
    "    rnd_sample = random.random()\n",
    "    \n",
    "    if rnd_sample <= EXPLORE_RATE:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = online_net.act(obs)\n",
    "    \n",
    "    new_obs, rew, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    transition = (obs, action, rew, done, new_obs)\n",
    "    replay_buffer.append(transition)\n",
    "    obs = new_obs\n",
    "\n",
    "    episode_reward += rew\n",
    "\n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "        obs = obs[0]\n",
    "\n",
    "        rew_buffer.append(episode_reward)\n",
    "        episode_reward = 0.0\n",
    "    \n",
    "    # After solved, watch it play\n",
    "    if len(rew_buffer) >= 100:\n",
    "        if np.mean(rew_buffer) >= 195:\n",
    "            while True:\n",
    "                action = online_net.act(obs)\n",
    "                \n",
    "                obs, _, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                env.render()\n",
    "                if done:\n",
    "                    env.reset()\n",
    "    \n",
    "    # Start Gradient Descent Step\n",
    "    transitions = random.sample(replay_buffer, BATCH_SIZE)\n",
    "\n",
    "    obses = np.asarray([t[0] for t in transitions])\n",
    "    actions = np.asarray([t[1] for t in transitions])\n",
    "    rews = np.asarray([t[2] for t in transitions])\n",
    "    dones = np.asarray([t[3] for t in transitions])\n",
    "    new_obses = np.asarray([t[4] for t in transitions])\n",
    "\n",
    "    obses_t = torch.as_tensor(obses, dtype=torch.float32)\n",
    "    actions_t = torch.as_tensor(actions, dtype=torch.int64).unsqueeze(-1)\n",
    "    rews_t = torch.as_tensor(rews, dtype=torch.float32).unsqueeze(-1)\n",
    "    dones_t = torch.as_tensor(dones, dtype=torch.float32).unsqueeze(-1)\n",
    "    new_obses_t = torch.as_tensor(new_obses, dtype=torch.float32)\n",
    "\n",
    "    # Compute Targets\n",
    "    target_q_values = target_net(new_obses_t)\n",
    "    max_target_q_values = target_q_values.max(dim=1, keepdim=True)[0]\n",
    "    \n",
    "    targets = rews_t + DISCOUNT_FACTOR *  (1 - dones_t) * max_target_q_values\n",
    "\n",
    "    # Compute Loss\n",
    "    q_values = online_net(obses_t)\n",
    "\n",
    "    action_q_values = torch.gather(input=q_values, dim=1, index=actions_t)\n",
    "\n",
    "    loss = nn.functional.smooth_l1_loss(action_q_values, targets)\n",
    "\n",
    "    # Gradient Descent\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Update Target Network\n",
    "    if step % TARGET_UPDATE_FREQ == 0:\n",
    "        target_net.load_state_dict(online_net.state_dict())\n",
    "    \n",
    "    # Logging\n",
    "    if step % 10000 == 0:\n",
    "        print()\n",
    "        print(\"Step\", step)\n",
    "        print(\"Avg Rew\", np.mean(rew_buffer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
