{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Qlearning Algorithm for OpenAI Gym Cartpole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import gym\n",
    "from collections import deque\n",
    "import itertools\n",
    "import numpy as np \n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCOUNT_FACTOR = 0.99\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 50000\n",
    "MIN_REPLAY_SIZE = 1000\n",
    "EXPLORE_RATE_START = 1.0\n",
    "EXPLORE_RATE_END = 0.02\n",
    "EXPLORE_RATE_DECAY = 10000\n",
    "TARGET_UPDATE_FREQ = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(2)\n",
      "State space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "Lower bounds of state space: [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n",
      "Upper bounds of state space: [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n"
     ]
    }
   ],
   "source": [
    "#env = gym.make(\"CartPole-v1\") \n",
    "env = gym.make(\"CartPole-v1\", render_mode= \"human\") # Use this one to visualize\n",
    "print(\"Action space:\", env.action_space)\n",
    "print(\"State space:\", env.observation_space)\n",
    "print(\"Lower bounds of state space:\", env.observation_space.low)\n",
    "print(\"Upper bounds of state space:\", env.observation_space.high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = deque(maxlen=BUFFER_SIZE)\n",
    "rew_buffer = deque([0.0], maxlen=100)\n",
    "\n",
    "episode_reward = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "\n",
    "        in_features = int(np.prod(env.observation_space.shape))\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, env.action_space.n)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    def act(self, obs):\n",
    "        obs_t = torch.as_tensor(obs, dtype=torch.float32)\n",
    "        q_values = self(obs_t.unsqueeze(0))\n",
    "\n",
    "        max_q_index = torch.argmax(q_values, dim=1)[0]\n",
    "        action = max_q_index.detach().item()\n",
    "\n",
    "        return action\n",
    "\n",
    "online_net = Network(env)\n",
    "target_net = Network(env)\n",
    "\n",
    "target_net.load_state_dict(online_net.state_dict())\n",
    "\n",
    "optimizer = torch.optim.Adam(online_net.parameters(), lr=5e4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-30 21:55:42.769 Python[43798:5137129] WARNING: Secure coding is not enabled for restorable state! Enable secure coding by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState: and returning YES.\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "obs = obs[0]\n",
    "for _ in range(MIN_REPLAY_SIZE):\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    new_obs, rew, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    transition = (obs, action, rew, done, new_obs)\n",
    "    replay_buffer.append(transition)\n",
    "    obs = new_obs\n",
    "\n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "        obs = obs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 0\n",
      "Avg Rew 0.0\n",
      "\n",
      "Step 1000\n",
      "Avg Rew 22.177777777777777\n",
      "\n",
      "Step 2000\n",
      "Avg Rew 22.211111111111112\n",
      "\n",
      "Step 3000\n",
      "Avg Rew 21.07\n",
      "\n",
      "Step 4000\n",
      "Avg Rew 19.56\n",
      "\n",
      "Step 5000\n",
      "Avg Rew 19.84\n",
      "\n",
      "Step 6000\n",
      "Avg Rew 22.31\n",
      "\n",
      "Step 7000\n",
      "Avg Rew 23.72\n",
      "\n",
      "Step 8000\n",
      "Avg Rew 24.09\n",
      "\n",
      "Step 9000\n",
      "Avg Rew 22.53\n",
      "\n",
      "Step 10000\n",
      "Avg Rew 24.6\n",
      "\n",
      "Step 11000\n",
      "Avg Rew 29.93\n",
      "\n",
      "Step 12000\n",
      "Avg Rew 35.65\n",
      "\n",
      "Step 13000\n",
      "Avg Rew 39.95\n",
      "\n",
      "Step 14000\n",
      "Avg Rew 44.26\n",
      "\n",
      "Step 15000\n",
      "Avg Rew 40.3\n",
      "\n",
      "Step 16000\n",
      "Avg Rew 42.36\n",
      "\n",
      "Step 17000\n",
      "Avg Rew 39.42\n",
      "\n",
      "Step 18000\n",
      "Avg Rew 33.09\n",
      "\n",
      "Step 19000\n",
      "Avg Rew 20.45\n",
      "\n",
      "Step 20000\n",
      "Avg Rew 15.34\n",
      "\n",
      "Step 21000\n",
      "Avg Rew 17.78\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/jacelu/Documents/ACL UROP/Reinforcement Learning Guide/Deep_Qlearning_P1.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacelu/Documents/ACL%20UROP/Reinforcement%20Learning%20Guide/Deep_Qlearning_P1.ipynb#X20sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacelu/Documents/ACL%20UROP/Reinforcement%20Learning%20Guide/Deep_Qlearning_P1.ipynb#X20sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     action \u001b[39m=\u001b[39m online_net\u001b[39m.\u001b[39mact(obs)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jacelu/Documents/ACL%20UROP/Reinforcement%20Learning%20Guide/Deep_Qlearning_P1.ipynb#X20sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m new_obs, rew, terminated, truncated, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacelu/Documents/ACL%20UROP/Reinforcement%20Learning%20Guide/Deep_Qlearning_P1.ipynb#X20sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m done \u001b[39m=\u001b[39m terminated \u001b[39mor\u001b[39;00m truncated\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jacelu/Documents/ACL%20UROP/Reinforcement%20Learning%20Guide/Deep_Qlearning_P1.ipynb#X20sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m transition \u001b[39m=\u001b[39m (obs, action, rew, done, new_obs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gym/wrappers/time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     40\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     51\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gym/wrappers/order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[1;32m     36\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gym/wrappers/env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, action)\n\u001b[1;32m     38\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gym/envs/classic_control/cartpole.py:187\u001b[0m, in \u001b[0;36mCartPoleEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    184\u001b[0m     reward \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m    186\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 187\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender()\n\u001b[1;32m    188\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32), reward, terminated, \u001b[39mFalse\u001b[39;00m, {}\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gym/envs/classic_control/cartpole.py:298\u001b[0m, in \u001b[0;36mCartPoleEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    297\u001b[0m     pygame\u001b[39m.\u001b[39mevent\u001b[39m.\u001b[39mpump()\n\u001b[0;32m--> 298\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclock\u001b[39m.\u001b[39;49mtick(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmetadata[\u001b[39m\"\u001b[39;49m\u001b[39mrender_fps\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    299\u001b[0m     pygame\u001b[39m.\u001b[39mdisplay\u001b[39m.\u001b[39mflip()\n\u001b[1;32m    301\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrgb_array\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "obs = obs[0]\n",
    "\n",
    "for step in itertools.count():\n",
    "    EXPLORE_RATE = np.interp(step, [0, EXPLORE_RATE_DECAY], [EXPLORE_RATE_START, EXPLORE_RATE_END])\n",
    "\n",
    "    rnd_sample = random.random()\n",
    "    \n",
    "    if rnd_sample <= EXPLORE_RATE:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = online_net.act(obs)\n",
    "    \n",
    "    new_obs, rew, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    transition = (obs, action, rew, done, new_obs)\n",
    "    replay_buffer.append(transition)\n",
    "    obs = new_obs\n",
    "\n",
    "    episode_reward += rew\n",
    "\n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "        obs = obs[0]\n",
    "\n",
    "        rew_buffer.append(episode_reward)\n",
    "        episode_reward = 0.0\n",
    "    \n",
    "    # After solved, watch it play\n",
    "    if len(rew_buffer) >= 100:\n",
    "        if np.mean(rew_buffer) >= 195:\n",
    "            while True:\n",
    "                action = online_net.act(obs)\n",
    "                \n",
    "                obs, _, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                env.render()\n",
    "                if done:\n",
    "                    env.reset()\n",
    "    \n",
    "    # Start Gradient Descent Step\n",
    "    transitions = random.sample(replay_buffer, BATCH_SIZE)\n",
    "\n",
    "    obses = np.asarray([t[0] for t in transitions])\n",
    "    actions = np.asarray([t[1] for t in transitions])\n",
    "    rews = np.asarray([t[2] for t in transitions])\n",
    "    dones = np.asarray([t[3] for t in transitions])\n",
    "    new_obses = np.asarray([t[4] for t in transitions])\n",
    "\n",
    "    obses_t = torch.as_tensor(obses, dtype=torch.float32)\n",
    "    actions_t = torch.as_tensor(actions, dtype=torch.int64).unsqueeze(-1)\n",
    "    rews_t = torch.as_tensor(rews, dtype=torch.float32).unsqueeze(-1)\n",
    "    dones_t = torch.as_tensor(dones, dtype=torch.float32).unsqueeze(-1)\n",
    "    new_obses_t = torch.as_tensor(new_obses, dtype=torch.float32)\n",
    "\n",
    "    # Compute Targets\n",
    "    target_q_values = target_net(new_obses_t)\n",
    "    max_target_q_values = target_q_values.max(dim=1, keepdim=True)[0]\n",
    "    \n",
    "    targets = rews_t + DISCOUNT_FACTOR *  (1 - dones_t) * max_target_q_values\n",
    "\n",
    "    # Compute Loss\n",
    "    q_values = online_net(obses_t)\n",
    "\n",
    "    action_q_values = torch.gather(input=q_values, dim=1, index=actions_t)\n",
    "\n",
    "    loss = nn.functional.smooth_l1_loss(action_q_values, targets)\n",
    "\n",
    "    # Gradient Descent\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Update Target Network\n",
    "    if step % TARGET_UPDATE_FREQ == 0:\n",
    "        target_net.load_state_dict(online_net.state_dict())\n",
    "    \n",
    "    # Logging\n",
    "    if step % 1000 == 0:\n",
    "        print()\n",
    "        print(\"Step\", step)\n",
    "        print(\"Avg Rew\", np.mean(rew_buffer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
